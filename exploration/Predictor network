import os
import torch
from torch import nn
from torch.utils.data import DataLoader
from torchvision import datasets, transforms
import numpy as np
import pandas as pd
from torch.utils.data import Dataset, DataLoader
import matplotlib.pyplot as plt
from numpy import array

#Generating random data to test training
data = {
    "embedding": [np.random.rand(256, 1) for _ in range(1000)],
    "score": np.random.rand(1000),
    }
dataframe = pd.DataFrame(data)
#
#
#Class DataPreparer prepares input data by taking embedding and scores
class DataPreparer(Dataset):
    def __init__(self, dataframe):
        self.dataframe = dataframe

    def __len__(self):
        return len(self.dataframe)

    def __getitem__(self, idx):
        # Extract the embedding and score
        embedding = self.dataframe.iloc[idx]["embedding"]
        score = self.dataframe.iloc[idx]["score"]
        
        # Convert to PyTorch tensors
        embedding_tensor = torch.tensor(embedding, dtype=torch.float32).squeeze()  # Flatten to (256,)
        score_tensor = torch.tensor(score, dtype=torch.float32)
        
        return embedding_tensor, score_tensor

# Create dataset object in correct formatting for input layer
dataset = DataPreparer(dataframe)
#
#

# Defining DataLoader
dataloader = DataLoader(dataset, batch_size=32, shuffle=True)

#Defines neural network and neural network architecture
class NeuralNetwork(nn.Module):
    def __init__(self):
        super().__init__()
        self.linear_relu_stack = nn.Sequential(
            nn.Linear(256, 128),
            nn.ReLU(),
            nn.Dropout(p=0.1),
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Dropout(p=0.1),
            nn.Linear(64, 1),
        )

    def forward(self, x):
        logits = self.linear_relu_stack(x)
        return logits
criterion = nn.MSELoss()

epoch_losses = []

model = NeuralNetwork()
optimiser = torch.optim.Adam(model.parameters(), lr=0.001)
epochs=20
for epoch in range(epochs):
    epoch_loss = 0.0
    for batch in dataloader:
        # Get the input embedding and target score
        inputs, targets = batch
        
        # Forward pass
        predictions = model(inputs)
        loss = criterion(predictions.squeeze(), targets)  # Squeeze to match dimensions
        
        # Backward pass and optimisation
        optimiser.zero_grad()
        loss.backward()
        optimiser.step()
        
        epoch_loss += loss.item()

    epoch_loss /= len(dataloader)
    epoch_losses.append(epoch_loss)
#
#
#Plots graph of losses at each epoch
def loss_plotter():
    epochlist=list(range(1, epochs+1))
    plt.plot(epochlist, epoch_losses)
    plt.show()

loss_plotter()